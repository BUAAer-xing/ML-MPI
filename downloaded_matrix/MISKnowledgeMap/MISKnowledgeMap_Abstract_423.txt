Conceptual modeling grammars are used to create scripts that represent someone's perception, or some group's negotiated perception, of domain semantics. For many years, researchers have evaluated conceptual modeling grammars to determine ways that they can be improved. One way to evaluate them is to empirically evaluate the strengths and weaknesses of the grammars in terms of their effectiveness and efficiency in generating scripts. A number of researchers have proposed guidelines for the design of empirical research to conduct such evaluations. Although these guidelines have proved useful, further clarification is needed in relation to (1) criteria for evaluating grammar performance, (2) characteristics of grammars that can influence grammar performance, and (3) factors that must be considered when testing the effect of grammar characteristics on grammar performance. We review past conceptual modeling research and provide guidelines for addressing these three issues. We also illustrate how the guidelines would apply to studies that evaluate conceptual modeling grammars from an ontological perspective. Finally, we discuss how the guidelines extend those offered in past research and the implications of our work for future research.
